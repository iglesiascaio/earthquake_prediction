{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834dbd3-ea41-463c-96ef-53497a31df77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Added to sys.path:\n",
      "  • /home/gridsan/mknuth/01_Seismic_Wave_Data_Prediction/02_Functions True\n",
      "  • /home/gridsan/mknuth/01_Seismic_Wave_Data_Prediction/02_Functions/seisLM/seisLM True\n",
      "✓ aliased toto.model  ➜  top-level 'model'\n",
      "Finished Importing\n",
      "Starting main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nk=2\\nTrainable params: 503,624\\nFrozen params:    162,665,648\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main script for earthquake magnitude prediction project.\n",
    "This script orchestrates the model training and evaluation process.\n",
    "\"\"\"\n",
    "print(\"Start\")\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import sys, pathlib, os\n",
    "INNER = pathlib.Path(\"~/toto/toto\").expanduser()\n",
    "if str(INNER) not in sys.path:\n",
    "    sys.path.insert(0, str(INNER))\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"     # stay offline\n",
    "\n",
    "\n",
    "# --- locate 02_Functions and add to path --------------------------------\n",
    "from pathlib import Path\n",
    "NB_ROOT = Path.cwd()\n",
    "\n",
    "FUNC_DIR  = NB_ROOT / \"02_Functions\"                          # ./02_Functions\n",
    "SEISLM_INNER = FUNC_DIR / \"seisLM\" / \"seisLM\"                 # ./02_Functions/seisLM/seisLM\n",
    "\n",
    "for p in (FUNC_DIR, SEISLM_INNER):\n",
    "    if p.is_dir() and str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "\n",
    "print(\"Added to sys.path:\")\n",
    "for p in (FUNC_DIR, SEISLM_INNER):\n",
    "    print(\"  •\", p if p.is_dir() else \"(missing)\", p.is_dir())\n",
    "\n",
    "# Make outer name 'seisLM' point to the inner package that *does* have .model\n",
    "if \"seisLM\" not in sys.modules:\n",
    "    inner_pkg = importlib.import_module(\"seisLM.seisLM\")\n",
    "    sys.modules[\"seisLM\"] = inner_pkg\n",
    "\n",
    "TOTO_INNER = pathlib.Path.home() / \"toto\" / \"toto\"\n",
    "if TOTO_INNER.is_dir() and str(TOTO_INNER) not in sys.path:\n",
    "    sys.path.insert(0, str(TOTO_INNER))\n",
    "\n",
    "# alias:  sys.modules[\"model\"]  ->  toto.model  (and its sub-modules)\n",
    "if \"model\" not in sys.modules:\n",
    "    sys.modules[\"model\"] = importlib.import_module(\"toto.model\")\n",
    "\n",
    "print(\"✓ aliased toto.model  ➜  top-level 'model'\")\n",
    "\n",
    "# Import project modules\n",
    "from Model_Trainer import ModelTrainer\n",
    "from Model_Evaluator import ModelEvaluator\n",
    "from Dataset_creation import create_train_dataset_new, get_data_loader\n",
    "print(\"Finished Importing\")\n",
    "\n",
    "# find config.yaml right next to the notebook / script\n",
    "PROJECT_DIR = Path.cwd()                           # current working dir\n",
    "CFG_CANDIDATE = PROJECT_DIR / \"config.yaml\"\n",
    "\n",
    "# if we launched one level up, fall back to sub-dir path\n",
    "if not CFG_CANDIDATE.is_file():\n",
    "    CFG_CANDIDATE = PROJECT_DIR / \"01_Seismic_Wave_Data_Prediction\" / \"config.yaml\"\n",
    "DEFAULT_CFG = str(CFG_CANDIDATE)\n",
    "\n",
    "\n",
    "def parse_args_jupyter_safe() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"Run training / evaluation\")\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        default=DEFAULT_CFG,         # ← here\n",
    "        help=\"Path to YAML config file\")\n",
    "    parser.add_argument(\n",
    "        \"--mode\", choices=[\"train\", \"evaluate\", \"both\"], default=\"both\")\n",
    "    parser.add_argument(\"--checkpoint\", default=None)\n",
    "\n",
    "    # Ignore stray notebook flags\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        args, _ = parser.parse_known_args()\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from YAML file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    #print(config)\n",
    "    return config\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate training and evaluation.\"\"\"\n",
    "    # Parse arguments and load configuration\n",
    "    args = parse_args_jupyter_safe()\n",
    "    config = load_config(args.config)\n",
    "    \n",
    "    # Setup output directories\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(config['paths']['output_dir'], f\"{config['model']['model_type']}_{config['model']['aggregation_type']}_{timestamp}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save configuration to output directory\n",
    "    with open(os.path.join(output_dir, 'config.yaml'), 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "        \n",
    "    # Check if using tabular features\n",
    "    use_tabular_features = config['model'].get('use_tabular_features', False)\n",
    "    \n",
    "    # Create dataset and data loader\n",
    "    #print(\"Creating dataset...\")\n",
    "    train_data = create_train_dataset_new(\n",
    "        config['paths']['earthquake_parquet'],\n",
    "        config['paths']['combined_stream_dir']\n",
    "    )\n",
    "    \n",
    "    print(train_data)\n",
    "    \n",
    "    train_data = train_data[:5]\n",
    "    data_loader = get_data_loader(\n",
    "        train_data,\n",
    "        config['paths']['earthquake_parquet'],  # For tabular features if needed\n",
    "        window_size_days=config['data']['window_size_days'],\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        use_tabular_features=use_tabular_features,\n",
    "        downsampling_rate = config['data']['downsampling_rate']\n",
    "    )\n",
    "    \n",
    "    # Determine device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Train model if requested\n",
    "    if args.mode in ['train', 'both']:\n",
    "        trainer = ModelTrainer(config, output_dir, device)\n",
    "        model, checkpoint_path = trainer.train(data_loader)\n",
    "    \n",
    "    # Evaluate model if requested\n",
    "    if args.mode in ['evaluate', 'both']:\n",
    "        # If we're only evaluating, load the model from checkpoint\n",
    "        if args.mode == 'evaluate':\n",
    "            checkpoint_path = args.checkpoint\n",
    "            if checkpoint_path is None:\n",
    "                print(\"Error: Checkpoint path required for evaluation mode\")\n",
    "                return\n",
    "            \n",
    "            trainer = ModelTrainer(config, output_dir, device)\n",
    "            model = trainer.load_model(checkpoint_path)\n",
    "        \n",
    "        # Setup evaluator\n",
    "        evaluator = ModelEvaluator(\n",
    "            model=model,\n",
    "            data_loader=data_loader,\n",
    "            device=device,\n",
    "            class_names=config['model']['class_names'],\n",
    "            output_dir=os.path.join(output_dir, 'evaluation')\n",
    "        )\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = evaluator.evaluate()\n",
    "        print(f\"Evaluation completed. Results saved to {os.path.join(output_dir, 'evaluation')}\")\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print('Starting main')\n",
    "    #main()\n",
    "    \n",
    "\"\"\"\n",
    "k=2\n",
    "Trainable params: 503,624\n",
    "Frozen params:    162,665,648\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8fbc78-b730-4d25-b880-509c5e5aaae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/gridsan/mknuth/01_Seismic_Wave_Data_Prediction/03_Results/seislm_lstm_20250729_192142/checkpoints/checkpoint_epoch1.pth'\n",
    "args = parse_args_jupyter_safe()\n",
    "\n",
    "config = load_config(args.config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(f\"Using device: {device}\")\n",
    "# Setup output directories\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = os.path.join(config['paths']['output_dir'], f\"{config['model']['model_type']}_{config['model']['aggregation_type']}_{timestamp}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "trainer = ModelTrainer(config, output_dir, device)\n",
    "\n",
    "model = trainer.load_model(checkpoint_path)\n",
    "\n",
    "# Create dataset and data loader\n",
    "#print(\"Creating dataset...\")\n",
    "train_data = create_train_dataset_new(\n",
    "        config['paths']['earthquake_parquet'],\n",
    "        config['paths']['combined_stream_dir']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d571fb4-41e7-4a45-8591-035b049050f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading pretrained SeisLM model...\n",
      "Freezing seislm model parameters...\n",
      "Initializing SeisLM model with lstm aggregation...\n",
      "Loading pretrained SeisLM model...\n",
      "Freezing seislm model parameters...\n",
      "Initializing SeisLM model with lstm aggregation...\n",
      "Loading model from checkpoint: /home/gridsan/mknuth/01_Seismic_Wave_Data_Prediction/03_Results/seislm_lstm_20250729_192142/checkpoints/checkpoint_epoch1.pth\n",
      "Loaded model from epoch 1 with loss 1.5627 and accuracy 0.4846\n",
      "Creating dataset...\n",
      "Creating train dataset …\n",
      "Folder contents: 9738 files\n",
      "SYN 2021: creating 48 samples\n",
      "[INFO] No magnitude found for station 'SYN' on 2021-07-31 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2021-11-27 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2021-12-04 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2021-12-11 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2021-12-18 – using PAS value instead.\n",
      "YUH2 2020: creating 49 samples\n",
      "STG 2022: creating 48 samples\n",
      "CYP 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'CYP' on 2024-12-31 – using PAS value instead.\n",
      "LVY 2023: creating 48 samples\n",
      "PTD 2023: creating 48 samples\n",
      "SES 2022: creating 48 samples\n",
      "RRX 2021: creating 48 samples\n",
      "[INFO] No magnitude found for station 'RRX' on 2021-01-30 – using PAS value instead.\n",
      "SDD 2023: creating 48 samples\n",
      "MAN 2023: creating 48 samples\n",
      "[INFO] No magnitude found for station 'MAN' on 2023-02-06 – using PAS value instead.\n",
      "SYN 2023: creating 48 samples\n",
      "[INFO] No magnitude found for station 'SYN' on 2023-10-02 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2023-11-13 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2023-11-20 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2023-11-27 – using PAS value instead.\n",
      "SDG 2022: creating 48 samples\n",
      "PASC 2022: creating 48 samples\n",
      "CPO 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'CPO' on 2024-12-31 – using PAS value instead.\n",
      "DGR 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'DGR' on 2024-12-31 – using PAS value instead.\n",
      "FMO 2023: creating 48 samples\n",
      "EML 2022: creating 48 samples\n",
      "DLA 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'DLA' on 2024-12-31 – using PAS value instead.\n",
      "PASC 2020: creating 49 samples\n",
      "SES 2021: creating 48 samples\n",
      "CPO 2023: creating 48 samples\n",
      "AGM 2023: creating 48 samples\n",
      "[INFO] No magnitude found for station 'AGM' on 2023-01-30 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'AGM' on 2023-02-06 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'AGM' on 2023-12-18 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'AGM' on 2023-12-25 – using PAS value instead.\n",
      "HYS 2023: creating 48 samples\n",
      "CYP 2020: creating 49 samples\n",
      "GOR 2023: creating 48 samples\n",
      "MAN 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'MAN' on 2024-02-13 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'MAN' on 2024-02-20 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'MAN' on 2024-09-10 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'MAN' on 2024-12-17 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'MAN' on 2024-12-24 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'MAN' on 2024-12-31 – using PAS value instead.\n",
      "BAI 2023: creating 48 samples\n",
      "BRE 2023: creating 48 samples\n",
      "SYN 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'SYN' on 2024-04-23 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2024-06-11 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2024-12-10 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2024-12-17 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2024-12-24 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2024-12-31 – using PAS value instead.\n",
      "SDG 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'SDG' on 2024-12-31 – using PAS value instead.\n",
      "CRR 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'CRR' on 2024-12-31 – using PAS value instead.\n",
      "WRC2 2021: creating 48 samples\n",
      "GOR 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'GOR' on 2024-12-31 – using PAS value instead.\n",
      "SES 2020: creating 49 samples\n",
      "BTP 2022: creating 48 samples\n",
      "FMO 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'FMO' on 2024-12-31 – using PAS value instead.\n",
      "SDD 2020: creating 49 samples\n",
      "HYS 2020: creating 49 samples\n",
      "PASC 2021: creating 48 samples\n",
      "BHP 2022: creating 48 samples\n",
      "STG 2023: creating 48 samples\n",
      "DGR 2021: creating 48 samples\n",
      "BRE 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'BRE' on 2024-12-31 – using PAS value instead.\n",
      "CRR 2022: creating 48 samples\n",
      "STG 2021: creating 48 samples\n",
      "SDD 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'SDD' on 2024-12-31 – using PAS value instead.\n",
      "BHP 2023: creating 48 samples\n",
      "DGR 2020: creating 49 samples\n",
      "RRX 2023: creating 48 samples\n",
      "[INFO] No magnitude found for station 'RRX' on 2023-02-27 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'RRX' on 2023-03-06 – using PAS value instead.\n",
      "RRX 2022: creating 48 samples\n",
      "ABL 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'ABL' on 2024-12-31 – using PAS value instead.\n",
      "BHP 2021: creating 48 samples\n",
      "SYN 2020: creating 49 samples\n",
      "[INFO] No magnitude found for station 'SYN' on 2020-09-24 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'SYN' on 2020-10-01 – using PAS value instead.\n",
      "SDG 2023: creating 48 samples\n",
      "YUH2 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'YUH2' on 2024-12-31 – using PAS value instead.\n",
      "SDG 2020: creating 49 samples\n",
      "PASC 2023: creating 48 samples\n",
      "BAI 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'BAI' on 2024-12-31 – using PAS value instead.\n",
      "WRC2 2020: creating 49 samples\n",
      "RRX 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'RRX' on 2024-02-20 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'RRX' on 2024-12-31 – using PAS value instead.\n",
      "SDG 2021: creating 48 samples\n",
      "CYP 2021: creating 48 samples\n",
      "AGM 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'AGM' on 2024-01-30 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'AGM' on 2024-02-06 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'AGM' on 2024-02-13 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'AGM' on 2024-02-20 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'AGM' on 2024-02-27 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'AGM' on 2024-12-24 – using PAS value instead.\n",
      "[INFO] No magnitude found for station 'AGM' on 2024-12-31 – using PAS value instead.\n",
      "STG 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'STG' on 2024-12-31 – using PAS value instead.\n",
      "SES 2023: creating 48 samples\n",
      "GOR 2020: creating 49 samples\n",
      "HYS 2024: creating 49 samples\n",
      "[INFO] No magnitude found for station 'HYS' on 2024-12-31 – using PAS value instead.\n",
      "Finished – total samples: 3246\n",
      "Initializing SeisLMWindowedDataset\n",
      "Window size (in days): 0.04166666666666666\n",
      "Starting comprehensive evaluation of SeisLMForMagnitudePrediction...\n",
      "Model does not use tabular features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|███       | 983/3246 [3:20:00<7:11:35, 11.44s/it] "
     ]
    }
   ],
   "source": [
    "def only_evaluate():\n",
    "    checkpoint_path= '/home/gridsan/mknuth/01_Seismic_Wave_Data_Prediction/03_Results/seislm_lstm_20250729_192142/checkpoints/checkpoint_epoch1.pth'\n",
    "    args = parse_args_jupyter_safe()\n",
    "\n",
    "    config = load_config(args.config)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    print(f\"Using device: {device}\")\n",
    "    # Setup output directories\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(config['paths']['output_dir'], f\"{config['model']['model_type']}_{config['model']['aggregation_type']}_{timestamp}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    trainer = ModelTrainer(config, output_dir, device)\n",
    "\n",
    "    model = trainer.load_model(checkpoint_path)\n",
    "\n",
    "    # Create dataset and data loader\n",
    "    print(\"Creating dataset...\")\n",
    "    train_data = create_train_dataset_new(\n",
    "            config['paths']['earthquake_parquet'],\n",
    "            config['paths']['combined_stream_dir']\n",
    "        )\n",
    "    use_tabular_features = config['model'].get('use_tabular_features', False)\n",
    "    data_loader = get_data_loader(\n",
    "            train_data,\n",
    "            config['paths']['earthquake_parquet'],  # For tabular features if needed\n",
    "            window_size_days=config['data']['window_size_days'],\n",
    "            batch_size=config['training']['batch_size'],\n",
    "            shuffle=True,\n",
    "            use_tabular_features=use_tabular_features\n",
    "        )     \n",
    "    # Setup evaluator\n",
    "    evaluator = ModelEvaluator(\n",
    "        model=model,\n",
    "        data_loader=data_loader,\n",
    "        device=device,\n",
    "        class_names=config['model']['class_names'],\n",
    "        output_dir=os.path.join(output_dir, 'evaluation')\n",
    "    )\n",
    "\n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate()\n",
    "    print(f\"Evaluation completed. Results saved to {os.path.join(output_dir, 'evaluation')}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #main()\n",
    "    only_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081d612-fdf4-4d16-9d44-49b758d735c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (toto_py310)",
   "language": "python",
   "name": "toto_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
